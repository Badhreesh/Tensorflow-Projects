{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing a CNN for MNIST Dataset Using Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_dataset\\train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_dataset\\train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_dataset\\t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_dataset\\t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist_data = input_data.read_data_sets(\"MNIST_dataset\", one_hot = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mnist variable created above contains both the images and their labels. Isolate the images for now. There will be 55,000 images and each will be of size 28 x 28."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55000, 784)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = mnist_data.train.images[:55000,:]\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at what a random image might look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x909080fda0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "randumNum = random.randint(0,55000)\n",
    "image = x_train[randumNum].reshape([28,28])\n",
    "plt.imshow(image, cmap=plt.get_cmap('gray_r'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Conv2D, MaxPool2D, Flatten, BatchNormalization\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, 3, activation='relu', input_shape=[28, 28, 1]))\n",
    "model.add(MaxPool2D())\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(64, 3, activation='relu'))\n",
    "model.add(MaxPool2D())\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(128, 3, activation='relu'))\n",
    "model.add(MaxPool2D())\n",
    "model.add(BatchNormalization())\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 13, 13, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 5, 5, 64)          256       \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 3, 3, 128)         73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 1, 1, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 1, 1, 128)         512       \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 129,162\n",
      "Trainable params: 128,714\n",
      "Non-trainable params: 448\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 55000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "55000/55000 [==============================] - 65s 1ms/step - loss: 0.1198 - acc: 0.9635 - val_loss: 0.0575 - val_acc: 0.9837\n",
      "Epoch 2/20\n",
      "55000/55000 [==============================] - 48s 880us/step - loss: 0.0490 - acc: 0.9844 - val_loss: 0.0900 - val_acc: 0.9750\n",
      "Epoch 3/20\n",
      "55000/55000 [==============================] - 46s 829us/step - loss: 0.0376 - acc: 0.9877 - val_loss: 0.0590 - val_acc: 0.9816\n",
      "Epoch 4/20\n",
      "55000/55000 [==============================] - 47s 846us/step - loss: 0.0277 - acc: 0.9910 - val_loss: 0.0462 - val_acc: 0.9877\n",
      "Epoch 5/20\n",
      "55000/55000 [==============================] - 46s 841us/step - loss: 0.0220 - acc: 0.9930 - val_loss: 0.0641 - val_acc: 0.9819\n",
      "Epoch 6/20\n",
      "55000/55000 [==============================] - 46s 833us/step - loss: 0.0220 - acc: 0.9925 - val_loss: 0.0495 - val_acc: 0.9859\n",
      "Epoch 7/20\n",
      "55000/55000 [==============================] - 46s 834us/step - loss: 0.0179 - acc: 0.9943 - val_loss: 0.0557 - val_acc: 0.9853\n",
      "Epoch 8/20\n",
      "55000/55000 [==============================] - 46s 837us/step - loss: 0.0162 - acc: 0.9947 - val_loss: 0.0359 - val_acc: 0.9901\n",
      "Epoch 9/20\n",
      "55000/55000 [==============================] - 47s 849us/step - loss: 0.0117 - acc: 0.9962 - val_loss: 0.0480 - val_acc: 0.9887\n",
      "Epoch 10/20\n",
      "55000/55000 [==============================] - 46s 845us/step - loss: 0.0147 - acc: 0.9952 - val_loss: 0.0523 - val_acc: 0.9872\n",
      "Epoch 11/20\n",
      "55000/55000 [==============================] - 46s 836us/step - loss: 0.0115 - acc: 0.9959 - val_loss: 0.0478 - val_acc: 0.9888\n",
      "Epoch 12/20\n",
      "55000/55000 [==============================] - 46s 828us/step - loss: 0.0119 - acc: 0.9961 - val_loss: 0.0496 - val_acc: 0.9892\n",
      "Epoch 13/20\n",
      "55000/55000 [==============================] - 45s 812us/step - loss: 0.0076 - acc: 0.9976 - val_loss: 0.0476 - val_acc: 0.9900\n",
      "Epoch 14/20\n",
      "55000/55000 [==============================] - 47s 856us/step - loss: 0.0101 - acc: 0.9965 - val_loss: 0.0645 - val_acc: 0.9869\n",
      "Epoch 15/20\n",
      "55000/55000 [==============================] - 47s 855us/step - loss: 0.0072 - acc: 0.9973 - val_loss: 0.0694 - val_acc: 0.9869\n",
      "Epoch 16/20\n",
      "55000/55000 [==============================] - 46s 841us/step - loss: 0.0083 - acc: 0.9973 - val_loss: 0.0531 - val_acc: 0.9897\n",
      "Epoch 17/20\n",
      "55000/55000 [==============================] - 46s 832us/step - loss: 0.0083 - acc: 0.9974 - val_loss: 0.0604 - val_acc: 0.9886\n",
      "Epoch 18/20\n",
      "55000/55000 [==============================] - 46s 845us/step - loss: 0.0088 - acc: 0.9972 - val_loss: 0.0652 - val_acc: 0.9867\n",
      "Epoch 19/20\n",
      "55000/55000 [==============================] - 46s 832us/step - loss: 0.0062 - acc: 0.9980 - val_loss: 0.0594 - val_acc: 0.9886\n",
      "Epoch 20/20\n",
      "55000/55000 [==============================] - 48s 879us/step - loss: 0.0070 - acc: 0.9979 - val_loss: 0.0830 - val_acc: 0.9839\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x909347fb70>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 20\n",
    "train = mnist_data.train.images\n",
    "train_images = np.reshape(train, (-1, 28, 28, 1))\n",
    "test = mnist_data.test.images\n",
    "test_images = np.reshape(test, (-1, 28, 28, 1))\n",
    "model.fit(train_images, mnist_data.train.labels,\n",
    "         validation_data=(test_images, mnist_data.test.labels),\n",
    "         epochs=epochs, batch_size=64, verbose=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
