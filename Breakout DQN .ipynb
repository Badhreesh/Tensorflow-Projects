{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "from gym.wrappers import Monitor\n",
    "import os\n",
    "import random\n",
    "import itertools\n",
    "import sys\n",
    "import psutil #library for retrieving information on running processes and system utilization\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Breakout-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ATARI Actions: 0 (noop), 1(fire), 2(left), 3(right) are valid actions\n",
    "VALID_ACTIONS = [0, 1, 2, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StateProcessor():\n",
    "    '''\n",
    "    Processes raw atari images. First convert to grayscale and then resize\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        # Build the Tensorflow graph\n",
    "        with tf.variable_scope('state_processor'):\n",
    "            self.input_state = tf.placeholder(shape=[210, 160, 3], dtype=tf.uint8)\n",
    "            self.output = tf.image.rgb_to_grayscale(self.input_state)\n",
    "            self.output = tf.image.crop_to_bounding_box(self.output, 34, 0, 160, 160)# Why 34, 0 are needed?\n",
    "            self.output = tf.image.resize_images(self.output,\n",
    "                [84, 84], method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "            self.output = tf.squeeze(self.output)\n",
    "    \n",
    "    def process(self, sess, state):\n",
    "        '''\n",
    "        Args:\n",
    "            sess: A TensorFLow session Object\n",
    "            state: A [210, 160, 3] Atari RGB State\n",
    "        \n",
    "        Returns:\n",
    "            A processed [84, 84] state representing grayscale values\n",
    "        '''\n",
    "        return sess.run(self.output, feed_dict = {self.input_state: state})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network():\n",
    "    '''\n",
    "    Q-Value Estimator Neural Network\n",
    "    This Network will be used to create the Q-Network and the Target Network\n",
    "    '''\n",
    "    def __init__(self, scope='network', summaries_dir=None):\n",
    "        self.scope = scope\n",
    "        #Writes TensorBoard summaries to disk\n",
    "        self.summary_writer = None\n",
    "        with tf.variable_scope(scope):\n",
    "            #Build the graph\n",
    "            self.build_graph()\n",
    "            if summaries_dir:\n",
    "                summary_dir = os.path.join(summaries_dir, \"summaries_{}\".format(scope))\n",
    "                if not os.path.exists(summary_dir):\n",
    "                    os.makedirs(summary_dir)\n",
    "                self.summary_writer = tf.summary.FileWriter(summary_dir)\n",
    "    \n",
    "    def build_graph(self):\n",
    "        '''\n",
    "        Builds the Tensorflow Graph\n",
    "        '''\n",
    "        \n",
    "        # Placeholders for our inputs\n",
    "        # Input is 4 RGB frames of shape [84, 84] each\n",
    "        self.X_ph = tf.placeholder(shape=[None, 84, 84, 4], dtype=tf.uint8, name='X')\n",
    "        \n",
    "        #The TD Target value\n",
    "        self.y_ph = tf.placeholder(shape=[None], dtype=tf.float32, name='y')\n",
    "        \n",
    "        #Integer id of which action was selected\n",
    "        self.actions_ph = tf.placeholder(shape=[None], dtype=tf.int32, name='actions')\n",
    "\n",
    "        batch_size = tf.shape(self.X_ph)[0]\n",
    "        \n",
    "        #Normalize the input\n",
    "        X = tf.to_float(self.X_ph)/255.0\n",
    "                \n",
    "        # 3 convolutional layers\n",
    "        conv1 = tf.layers.conv2d(inputs=X, filters=32, kernel_size=8, strides=4, activation=tf.nn.relu)\n",
    "        conv2 = tf.layers.conv2d(inputs=conv1, filters=64, kernel_size=4, strides=2, activation=tf.nn.relu)\n",
    "        conv3 = tf.layers.conv2d(inputs=conv2, filters=64, kernel_size=3, strides=1, activation=tf.nn.relu)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        flatten = tf.layers.flatten(conv3)\n",
    "        fc1 = tf.layers.dense(flatten, 512, activation=tf.nn.relu)\n",
    "        self.predictions = tf.layers.dense(fc1, len(VALID_ACTIONS), activation=tf.nn.relu)\n",
    "        \n",
    "        # Get the predictions for the chosen actions only\n",
    "        gather_indices = tf.range(batch_size) * tf.shape(self.predictions)[1] + self.actions_ph\n",
    "        self.action_predictions = tf.gather(tf.reshape(self.predictions, [-1]), gather_indices)\n",
    "        \n",
    "        # Calculate the loss\n",
    "        self.losses = tf.squared_difference(self.y_ph, self.action_predictions)\n",
    "        self.loss = tf.reduce_mean(self.losses)\n",
    "        \n",
    "        # Optimizer parameters from original paper \n",
    "        self.optimizer = tf.train.RMSPropOptimizer(learning_rate = 0.00025,\n",
    "                                                   decay = 0.99,\n",
    "                                                   momentum = 0.0,\n",
    "                                                   epsilon = 1e-6)\n",
    "        self.train_op = self.optimizer.minimize(self.loss, global_step=tf.train.get_global_step())\n",
    "        \n",
    "        # Summaries for Tensorboard\n",
    "        self.summaries = tf.summary.merge([\n",
    "            tf.summary.scalar('loss', self.loss),\n",
    "            tf.summary.histogram('loss_hist', self.losses),\n",
    "            tf.summary.histogram('q_values_hist', self.predictions),\n",
    "            tf.summary.scalar('max_q_value', tf.reduce_max(self.predictions))\n",
    "        ])\n",
    "    \n",
    "    def predict(self, sess, s):\n",
    "        '''\n",
    "        Predicts action values\n",
    "        \n",
    "        Args:\n",
    "            sess: Tensorflow session object\n",
    "            s: Input images of shape [batch_size, 84, 84, 4]\n",
    "            \n",
    "        Returns:\n",
    "            A tensor of shape [batch_size, NUM_VALID_ACTIONS] containing the estimated action values\n",
    "        '''\n",
    "        Q_vals = sess.run(self.predictions, feed_dict={self.X_ph: s})\n",
    "        return Q_vals\n",
    "    \n",
    "    def update(self, sess, s, a, y):\n",
    "        '''\n",
    "        Updates the model towards the given target\n",
    "        \n",
    "        Args:\n",
    "            sess: Tensorflow session object\n",
    "            s: Input images of shape [batch_size, 84, 84, 4]\n",
    "            a: Chosen actions of shape [batch_size]\n",
    "            y: Targets of shape [batch_size]\n",
    "        \n",
    "        Returns:\n",
    "            The calculated loss on the batch\n",
    "        '''\n",
    "        feed_dict = {self.X_ph: s, self.actions_ph: a, self.y_ph: y}\n",
    "        summaries, global_step, _, loss = sess.run(\n",
    "            [self.summaries, tf.train.get_global_step(), self.train_op, self.loss],\n",
    "            feed_dict = feed_dict)\n",
    "        if self.summary_writer:\n",
    "            self.summary_writer.add_summary(summaries, global_step)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/badhreesh/.local/share/virtualenvs/deeplearning/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 100.0\n",
      "Q Values for all actions: \n",
      " [[0.04632431 0.         0.0071764  0.        ]\n",
      " [0.04632431 0.         0.0071764  0.        ]]\n",
      "Q Values stretched out: \n",
      " [0.04632431 0.         0.0071764  0.         0.04632431 0.\n",
      " 0.0071764  0.        ]\n",
      "Actions based on which Q Values are be selected from stretched out array: [1 7]\n",
      "Selected Q Values based on actions are: [0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# For testing...\n",
    "\n",
    "tf.reset_default_graph()\n",
    "global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "\n",
    "test_model = Network(scope='test')\n",
    "sp = StateProcessor()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Example Observation Batch\n",
    "    observation = env.reset()\n",
    "    processed_obs = sp.process(sess, observation)\n",
    "    stacked_obs = np.stack([processed_obs]*4, axis=2)\n",
    "    observations = np.array([stacked_obs] * 2) # Assume a batch_size of 2\n",
    "    \n",
    "    # Test prediction\n",
    "    predictions = test_model.predict(sess, observations)\n",
    "    \n",
    "    \n",
    "    # Test training step\n",
    "    y = np.array([10.0, 10.0])\n",
    "    a = np.array([1, 3])\n",
    "    loss = test_model.update(sess, observations, a, y)\n",
    "    \n",
    "    #gather_indices is calculated to account for change in action indices when the array is stretched out\n",
    "    gather_indices = sess.run(tf.range(2) * tf.shape(test_model.predict(sess, observations)[1])) + a\n",
    "    print(\"Loss:\", loss)\n",
    "    print(\"Q Values for all actions: \\n\", predictions)\n",
    "    print(\"Q Values stretched out: \\n\", sess.run(tf.reshape(predictions, [-1])))\n",
    "    print('Actions based on which Q Values are be selected from stretched out array:', gather_indices)\n",
    "    print('Selected Q Values based on actions are:',\n",
    "          sess.run(tf.gather(tf.reshape(predictions, [-1]), gather_indices)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class update_target_network():\n",
    "    '''\n",
    "    Copy Q Network paramters to Target Network\n",
    "    '''\n",
    "    def __init__(self, model1, model2):\n",
    "        '''\n",
    "        Defines the copy graph\n",
    "        Args:\n",
    "            model1: model to copy paramters from (DQNetwork)\n",
    "            model2: model to copy paramters to (TargetNetwork)\n",
    "        '''\n",
    "        \n",
    "        #Get parameters of the Q Network\n",
    "        from_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,model1.scope)\n",
    "        \n",
    "        # Get paramters of the Target Network\n",
    "        to_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, model2.scope)\n",
    "        \n",
    "        self.update_ops = []\n",
    "        \n",
    "        for from_var, to_var in zip(from_vars, to_vars):\n",
    "            self.update_ops.append(to_var.assign(from_var))\n",
    "    \n",
    "    def make(self, sess):\n",
    "        '''\n",
    "        Makes the copy\n",
    "        Args:\n",
    "            sess: Tensorflow session object\n",
    "        '''\n",
    "        sess.run(self.update_ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_epsilon_greedy_policy(model, nA):\n",
    "    '''\n",
    "    Creates an spilon greedy policy based on the Q model and epsilon\n",
    "    \n",
    "    Args:\n",
    "        model: The model that returns the Q values for a given state\n",
    "        nA: Number of actions\n",
    "    \n",
    "    Returns:\n",
    "        \n",
    "    '''\n",
    "    def policy_fn(sess, observation, epsilon):\n",
    "        A = np.ones(nA, dtype=float) * epsilon / nA\n",
    "        #print(np.shape(np.expand_dims(observation,0)[0]))\n",
    "        q_values = model.predict(sess, np.expand_dims(observation, 0))[0]\n",
    "        best_action = np.argmax(q_values)\n",
    "        A[best_action] += (1.0 - epsilon)\n",
    "        return A\n",
    "    return policy_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deep_q_learning(sess,\n",
    "                   env,\n",
    "                   q_model,\n",
    "                   target_model,\n",
    "                   state_processor,\n",
    "                   num_episodes,\n",
    "                   experiment_dir,\n",
    "                   replay_memory_size = 500000,\n",
    "                   replay_memory_init_size = 50000,\n",
    "                   update_target_model_every = 10000,\n",
    "                   gamma = 0.99,\n",
    "                   epsilon_start = 1.0,\n",
    "                   epsilon_end = 0.1,\n",
    "                   epsilon_decay_steps = 500000,\n",
    "                   batch_size = 32,\n",
    "                   record_video_every = 50):\n",
    "    '''\n",
    "    Q Learning alg for off policy TD learning using function approximation\n",
    "    Finds the optimal greedy policy while following an epsilon greedy policy\n",
    "    \n",
    "    Args:\n",
    "    Returns:\n",
    "        An EpisodeStats object with 2 numpy arrays for episode_lengths and episode_rewards\n",
    "    '''\n",
    "    EpisodeStats = namedtuple(\"Stats\", [\"episode_lengths\", \"episode_rewards\"])\n",
    "    Transition = namedtuple(\"Transition\", [\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "    \n",
    "    # The replay memory\n",
    "    replay_memory = []\n",
    "    \n",
    "    # Make model copier object \n",
    "    copy_params = update_target_network(q_model, target_model)\n",
    "    \n",
    "    # Keep track of useful statistics\n",
    "    stats = EpisodeStats(episode_lengths=np.zeros(num_episodes),\n",
    "                        episode_rewards=np.zeros(num_episodes))\n",
    "    \n",
    "    # For system summaries, useful to check if current process looks healthy\n",
    "    current_process = psutil.Process()\n",
    "    \n",
    "    # Create directories for checkpoints and summaries\n",
    "    checkpoint_dir = os.path.join(experiment_dir, 'checkpoints')\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, 'model')\n",
    "    monitor_path = os.path.join(experiment_dir, 'monitor')\n",
    "    \n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "    if not os.path.exists(monitor_path):\n",
    "        os.makedirs(monitor_path)\n",
    "        \n",
    "    saver = tf.train.Saver()\n",
    "    # Load a previous checkpoint if we find one\n",
    "    latest_checkpoint = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "    if latest_checkpoint:\n",
    "        print('Loading model checkpoint {}...\\n'.format(latest_checkpoint))\n",
    "        saver.restore(sess, latest_checkpoint)\n",
    "    \n",
    "    # Get the current time step\n",
    "    total_t = sess.run(tf.train.get_global_step())\n",
    "    \n",
    "    # The epsilon decay schedule\n",
    "    epsilons = np.linspace(epsilon_start, epsilon_end, epsilon_decay_steps)\n",
    "    \n",
    "    # The policy we are following\n",
    "    policy = make_epsilon_greedy_policy(q_model, len(VALID_ACTIONS))\n",
    "    \n",
    "    # Populate the replay memory with initial experience\n",
    "    print('Populating replay memory')\n",
    "    state = env.reset()\n",
    "    state = state_processor.process(sess, state)\n",
    "    state = np.stack([state] * 4, axis=2)\n",
    "    for i in range(replay_memory_init_size):\n",
    "        #print(i)\n",
    "        action_probs = policy(sess, state, epsilons[min(total_t, epsilon_decay_steps-1)])\n",
    "        action = np.random.choice(np.arange(len(action_probs)), p= action_probs)\n",
    "        next_state, reward, done, _ = env.step(VALID_ACTIONS[action])\n",
    "        next_state = state_processor.process(sess, next_state)\n",
    "        next_state = np.append(state[:,:,1:], np.expand_dims(next_state, 2), axis=2)\n",
    "        replay_memory.append(Transition(state, action, reward, next_state, done))\n",
    "        if done:\n",
    "            state = env.reset()\n",
    "            state = state_processor.process(sess, state)\n",
    "            state = np.stack([state] * 4, axis=2)\n",
    "        else:\n",
    "            state = next_state\n",
    "\n",
    "    # Record videos\n",
    "    # Add env Monitor wrapper\n",
    "    env = Monitor(env, directory=monitor_path, video_callable=lambda count: count % record_video_every\n",
    "                  == 0, resume= True)\n",
    "        \n",
    "    for i_episode in range(num_episodes):\n",
    "            \n",
    "        # Save the current checkpoint\n",
    "        saver.save(tf.get_default_session(), checkpoint_path)\n",
    "            \n",
    "        # Reset the environment\n",
    "        state = env.reset()\n",
    "        state = state_processor.process(sess, state)\n",
    "        state = np.stack([state] * 4, axis=2)\n",
    "        loss = None\n",
    "            \n",
    "        # One step in the environment\n",
    "        #itertools.counts() iterates infinitely, until break \n",
    "        for t in itertools.count(): \n",
    "               \n",
    "            # Compute epsilon for this time step\n",
    "            epsilon = epsilons[min(total_t, epsilon_decay_steps-1)]\n",
    "                \n",
    "            # Update target network\n",
    "            if total_t % update_target_model_every == 0:\n",
    "                copy_params.make(sess)\n",
    "                print('\\nTarget Network Updated')\n",
    "                \n",
    "            # Print out which step we r on, useful for debugging\n",
    "            print('\\rStep {} ({}) @ Episode {}/{}, loss: {}'.format(\n",
    "                t, total_t, i_episode+1, num_episodes, loss), end=\"\")\n",
    "            sys.stdout.flush()\n",
    "                \n",
    "            #Take a step\n",
    "            action_probs = policy(sess, state, epsilons[min(total_t, epsilon_decay_steps-1)])\n",
    "            action = np.random.choice(np.arange(len(action_probs)), p= action_probs)\n",
    "            next_state, reward, done, _ = env.step(VALID_ACTIONS[action])\n",
    "            next_state = state_processor.process(sess, next_state)\n",
    "            next_state = np.append(state[:,:,1:], np.expand_dims(next_state, 2), axis=2)\n",
    "                \n",
    "            # If our reply memory is full, pop the first element\n",
    "            if len(replay_memory) == replay_memory_size:\n",
    "                replay_memory.pop(0)\n",
    "                \n",
    "            # Save transition to replay memory\n",
    "            replay_memory.append(Transition(state, action, reward, next_state, done))\n",
    "                \n",
    "            # Update statistics\n",
    "            stats.episode_rewards[i_episode] += reward\n",
    "            stats.episode_lengths[i_episode] += t\n",
    "                \n",
    "            # Sample a minibatch from the replay memory\n",
    "            samples = random.sample(replay_memory, batch_size)\n",
    "            states_batch, actions_batch, rewards_batch, next_states_batch, dones_batch = map(np.array, \n",
    "                                                                                                zip(*samples))\n",
    "                \n",
    "            # Calculate Q values and targets\n",
    "            q_values_next = target_model.predict(sess, next_states_batch)\n",
    "            targets_batch = rewards_batch + np.invert(dones_batch).astype(np.float32) * gamma * np.amax(q_values_next, axis=1)\n",
    "                \n",
    "            # Perform gradient descent update\n",
    "            states_batch = np.array(states_batch)\n",
    "            loss = q_model.update(sess, states_batch, actions_batch, targets_batch)\n",
    "                \n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "            state = next_state\n",
    "            total_t += 1\n",
    "\n",
    "        # Add summaries to tensorboard\n",
    "        episode_summary = tf.Summary()\n",
    "        episode_summary.value.add(simple_value=epsilon, tag=\"episode/epsilon\")\n",
    "        episode_summary.value.add(simple_value=stats.episode_rewards[i_episode], tag=\"episode/reward\")\n",
    "        episode_summary.value.add(simple_value=stats.episode_lengths[i_episode], tag=\"episode/length\")\n",
    "        episode_summary.value.add(simple_value=current_process.cpu_percent(), tag=\"system/cpu_usage_percent\")\n",
    "        episode_summary.value.add(simple_value=current_process.memory_percent(memtype=\"vms\"), tag=\"system/v_memeory_usage_percent\")\n",
    "        q_model.summary_writer.add_summary(episode_summary, i_episode)\n",
    "        q_model.summary_writer.flush()\n",
    "        \n",
    "        yield total_t, EpisodeStats(\n",
    "            episode_lengths=stats.episode_lengths[:i_episode+1],\n",
    "            episode_rewards=stats.episode_rewards[:i_episode+1])\n",
    "        \n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/badhreesh/.local/share/virtualenvs/deeplearning/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating replay memory\n",
      "\n",
      "Target Network Updated\n",
      "Step 228 (228) @ Episode 1/10000, loss: 4.7760153393028304e-05\n",
      "Episode Reward: 1.0\n",
      "Step 274 (502) @ Episode 2/10000, loss: 2.5199138690368272e-05\n",
      "Episode Reward: 2.0\n",
      "Step 234 (736) @ Episode 3/10000, loss: 4.92715735163074e-0555\n",
      "Episode Reward: 2.0\n",
      "Step 171 (907) @ Episode 4/10000, loss: 0.00025852100225165486\n",
      "Episode Reward: 0.0\n",
      "Step 281 (1188) @ Episode 5/10000, loss: 0.00030659703770652413\n",
      "Episode Reward: 2.0\n",
      "Step 284 (1472) @ Episode 6/10000, loss: 8.309585973620415e-054\n",
      "Episode Reward: 2.0\n",
      "Step 232 (1704) @ Episode 7/10000, loss: 1.1474141501821578e-05\n",
      "Episode Reward: 1.0\n",
      "Step 203 (1907) @ Episode 8/10000, loss: 2.3410702851833776e-05\n",
      "Episode Reward: 1.0\n",
      "Step 185 (2092) @ Episode 9/10000, loss: 1.382572554575745e-055\n",
      "Episode Reward: 0.0\n",
      "Step 381 (2473) @ Episode 10/10000, loss: 2.7261648938292637e-05\n",
      "Episode Reward: 3.0\n",
      "Step 249 (2722) @ Episode 11/10000, loss: 2.1620740881189704e-05\n",
      "Episode Reward: 1.0\n",
      "Step 176 (2898) @ Episode 12/10000, loss: 7.46955192880705e-0558\n",
      "Episode Reward: 0.0\n",
      "Step 262 (3160) @ Episode 13/10000, loss: 1.6979955034912564e-05\n",
      "Episode Reward: 2.0\n",
      "Step 264 (3424) @ Episode 14/10000, loss: 1.2171313755970914e-05\n",
      "Episode Reward: 2.0\n",
      "Step 275 (3699) @ Episode 15/10000, loss: 0.03110618516802787855\n",
      "Episode Reward: 2.0\n",
      "Step 183 (3882) @ Episode 16/10000, loss: 5.2681432862300426e-05\n",
      "Episode Reward: 0.0\n",
      "Step 279 (4161) @ Episode 17/10000, loss: 1.6894744476303458e-05\n",
      "Episode Reward: 2.0\n",
      "Step 174 (4335) @ Episode 18/10000, loss: 3.311128602945246e-055\n",
      "Episode Reward: 0.0\n",
      "Step 279 (4614) @ Episode 19/10000, loss: 1.2575058462971356e-05\n",
      "Episode Reward: 2.0\n",
      "Step 176 (4790) @ Episode 20/10000, loss: 5.721286288462579e-055\n",
      "Episode Reward: 0.0\n",
      "Step 182 (4972) @ Episode 21/10000, loss: 2.01231669052504e-0505\n",
      "Episode Reward: 0.0\n",
      "Step 181 (5153) @ Episode 22/10000, loss: 3.3088224881794304e-05\n",
      "Episode Reward: 0.0\n",
      "Step 267 (5420) @ Episode 23/10000, loss: 4.2482912249397486e-05\n",
      "Episode Reward: 2.0\n",
      "Step 166 (5586) @ Episode 24/10000, loss: 6.131274858489633e-055\n",
      "Episode Reward: 0.0\n",
      "Step 225 (5811) @ Episode 25/10000, loss: 1.577826333232224e-055\n",
      "Episode Reward: 1.0\n",
      "Step 211 (6022) @ Episode 26/10000, loss: 3.8573249184992164e-05\n",
      "Episode Reward: 1.0\n",
      "Step 356 (6378) @ Episode 27/10000, loss: 5.567078915191814e-055\n",
      "Episode Reward: 3.0\n",
      "Step 428 (6806) @ Episode 28/10000, loss: 2.664406201802194e-055\n",
      "Episode Reward: 5.0\n",
      "Step 207 (7013) @ Episode 29/10000, loss: 4.070323848281987e-055\n",
      "Episode Reward: 1.0\n",
      "Step 168 (7181) @ Episode 30/10000, loss: 0.03112180531024933054\n",
      "Episode Reward: 0.0\n",
      "Step 189 (7370) @ Episode 31/10000, loss: 3.3560776500962675e-05\n",
      "Episode Reward: 0.0\n",
      "Step 239 (7609) @ Episode 32/10000, loss: 0.0309525765478611-055\n",
      "Episode Reward: 1.0\n",
      "Step 353 (7962) @ Episode 33/10000, loss: 4.844189606956206e-055\n",
      "Episode Reward: 3.0\n",
      "Step 226 (8188) @ Episode 34/10000, loss: 2.3411410438711755e-05\n",
      "Episode Reward: 1.0\n",
      "Step 185 (8373) @ Episode 35/10000, loss: 2.0673498511314392e-05\n",
      "Episode Reward: 0.0\n",
      "Step 218 (8591) @ Episode 36/10000, loss: 0.00015496884589083493\n",
      "Episode Reward: 1.0\n",
      "Step 174 (8765) @ Episode 37/10000, loss: 3.515108619467355e-055\n",
      "Episode Reward: 0.0\n",
      "Step 194 (8959) @ Episode 38/10000, loss: 1.5149184946494643e-05\n",
      "Episode Reward: 0.0\n",
      "Step 184 (9143) @ Episode 39/10000, loss: 2.210332968388684e-055\n",
      "Episode Reward: 0.0\n",
      "Step 221 (9364) @ Episode 40/10000, loss: 0.00010356749407947063\n",
      "Episode Reward: 1.0\n",
      "Step 275 (9639) @ Episode 41/10000, loss: 2.3176420654635876e-05\n",
      "Episode Reward: 2.0\n",
      "Step 358 (9997) @ Episode 42/10000, loss: 5.299561598803848e-055\n",
      "Episode Reward: 3.0\n",
      "Step 2 (9999) @ Episode 43/10000, loss: 3.205694883945398e-05\n",
      "Target Network Updated\n",
      "Step 169 (10166) @ Episode 43/10000, loss: 2.770097489701584e-055\n",
      "Episode Reward: 0.0\n",
      "Step 297 (10463) @ Episode 44/10000, loss: 2.4197261154768057e-05\n",
      "Episode Reward: 2.0\n",
      "Step 166 (10629) @ Episode 45/10000, loss: 4.541678208624944e-055\n",
      "Episode Reward: 0.0\n",
      "Step 244 (10873) @ Episode 46/10000, loss: 4.05463288188912e-0555\n",
      "Episode Reward: 1.0\n",
      "Step 261 (11134) @ Episode 47/10000, loss: 7.474714220734313e-055\n",
      "Episode Reward: 2.0\n",
      "Step 273 (11407) @ Episode 48/10000, loss: 2.1956744603812695e-05\n",
      "Episode Reward: 2.0\n",
      "Step 178 (11585) @ Episode 49/10000, loss: 3.0274791242845822e-06\n",
      "Episode Reward: 0.0\n",
      "Step 248 (11833) @ Episode 50/10000, loss: 7.132613518479047e-065\n",
      "Episode Reward: 1.0\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 12] Cannot allocate memory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-67-49ca1137b9e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m                                    \u001b[0mepsilon_decay_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m500000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m                                    \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m                                    record_video_every = 50):\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\nEpisode Reward: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepisode_rewards\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-63-ff036127adf5>\u001b[0m in \u001b[0;36mdeep_q_learning\u001b[0;34m(sess, env, q_model, target_model, state_processor, num_episodes, experiment_dir, replay_memory_size, replay_memory_init_size, update_target_model_every, gamma, epsilon_start, epsilon_end, epsilon_decay_steps, batch_size, record_video_every)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;31m# Reset the environment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate_processor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/deeplearning/lib/python3.5/site-packages/gym/wrappers/monitor.py\u001b[0m in \u001b[0;36mreset\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_before_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mobservation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_after_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/deeplearning/lib/python3.5/site-packages/gym/wrappers/monitor.py\u001b[0m in \u001b[0;36m_after_reset\u001b[0;34m(self, observation)\u001b[0m\n\u001b[1;32m    191\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats_recorder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mafter_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_video_recorder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0;31m# Bump *after* all reset activity has finished\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/deeplearning/lib/python3.5/site-packages/gym/wrappers/monitor.py\u001b[0m in \u001b[0;36mreset_video_recorder\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    212\u001b[0m             \u001b[0menabled\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_video_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         )\n\u001b[0;32m--> 214\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvideo_recorder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcapture_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_close_video_recorder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/deeplearning/lib/python3.5/site-packages/gym/wrappers/monitoring/video_recorder.py\u001b[0m in \u001b[0;36mcapture_frame\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    114\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_encode_ansi_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_encode_image_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/deeplearning/lib/python3.5/site-packages/gym/wrappers/monitoring/video_recorder.py\u001b[0m in \u001b[0;36m_encode_image_frame\u001b[0;34m(self, frame)\u001b[0m\n\u001b[1;32m    160\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_encode_image_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImageEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframes_per_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoder_version'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/deeplearning/lib/python3.5/site-packages/gym/wrappers/monitoring/video_recorder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, output_path, frame_shape, frames_per_sec)\u001b[0m\n\u001b[1;32m    254\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDependencyNotInstalled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\"\"Found neither the ffmpeg nor avconv executables. On OS X, you can install ffmpeg via `brew install ffmpeg`. On most Ubuntu variants, `sudo apt-get install ffmpeg` should do it. On Ubuntu 14.04, however, you'll need to install avconv with `sudo apt-get install libav-tools`.\"\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/deeplearning/lib/python3.5/site-packages/gym/wrappers/monitoring/video_recorder.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    286\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Starting ffmpeg with \"%s\"'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcmdline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'setsid'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m#setsid not present on Windows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcmdline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstdin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPIPE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreexec_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetsid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    289\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcmdline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstdin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPIPE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/subprocess.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds)\u001b[0m\n\u001b[1;32m    945\u001b[0m                                 \u001b[0mc2pread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc2pwrite\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m                                 \u001b[0merrread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrwrite\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m                                 restore_signals, start_new_session)\n\u001b[0m\u001b[1;32m    948\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m             \u001b[0;31m# Cleanup if the child failed starting.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/subprocess.py\u001b[0m in \u001b[0;36m_execute_child\u001b[0;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, start_new_session)\u001b[0m\n\u001b[1;32m   1488\u001b[0m                             \u001b[0merrread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrwrite\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1489\u001b[0m                             \u001b[0merrpipe_read\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrpipe_write\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1490\u001b[0;31m                             restore_signals, start_new_session, preexec_fn)\n\u001b[0m\u001b[1;32m   1491\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_child_created\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1492\u001b[0m                 \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 12] Cannot allocate memory"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Where we save our checkpoints and graphs\n",
    "experiment_dir = os.path.abspath('./experiments/{}'.format(env.spec.id))\n",
    "\n",
    "# Create a global step variable\n",
    "global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "\n",
    "# Create Models\n",
    "q_model = Network(scope='q_model', summaries_dir=experiment_dir)\n",
    "target_model = Network(scope='target_model')\n",
    "\n",
    "# State Processor\n",
    "state_processor = StateProcessor()\n",
    "\n",
    "# Run it!\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for t, stats in deep_q_learning(sess,\n",
    "                                   env,\n",
    "                                   q_model=q_model,\n",
    "                                   target_model=target_model,\n",
    "                                   state_processor=state_processor,\n",
    "                                   num_episodes = 10000,\n",
    "                                   experiment_dir = experiment_dir,\n",
    "                                   replay_memory_size = 500000,\n",
    "                                   replay_memory_init_size = 50000,\n",
    "                                   update_target_model_every = 10000,\n",
    "                                   gamma = 0.99,\n",
    "                                   epsilon_start = 1.0,\n",
    "                                   epsilon_end = 0.1,\n",
    "                                   epsilon_decay_steps = 500000,\n",
    "                                   batch_size = 32,\n",
    "                                   record_video_every = 50):\n",
    "        print('\\nEpisode Reward: {}'.format(stats.episode_rewards[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
